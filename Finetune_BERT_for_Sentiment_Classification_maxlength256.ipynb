{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b69d7f56-7ac3-4a40-9c50-a7084ba0f6cd",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Review [Finetune_BERT_for_Sentiment_Classification_maxlength128.ipynb](Finetune_BERT_for_Sentiment_Classification_maxlength128.ipynb) (with `max_length = 128`) to understand the data and the model setup.\n",
    "\n",
    "Hyperparameter choices in this notebook:\n",
    "- Batch size (`batch_size`) = 32\n",
    "- Learning rate (`lr`) = 2e-5\n",
    "- Number of epochs (`epochs`) = 4\n",
    "- **Maximum Sequence Length (`max_length`) = 256**\n",
    "\n",
    "## Results\n",
    "\n",
    "```\n",
    "epoch 1: Avg Loss 0.2471, Train Acc 0.8990, Val Acc 0.9232\n",
    "epoch 2: Avg Loss 0.1423, Train Acc 0.9479, Val Acc 0.9252\n",
    "epoch 3: Avg Loss 0.0778, Train Acc 0.9738, Val Acc 0.9262\n",
    "epoch 4: Avg Loss 0.0452, Train Acc 0.9864, Val Acc 0.9256\n",
    "```\n",
    "\n",
    "```\n",
    "Test Acc: 0.9226\n",
    "```\n",
    "We observe overfitting with epochs 3 and 4. Thus, we would recommend stopping at epoch 2. Notice that doubling max length resulted in an increase in accuracy by about `2%`. However, training time has increased by about a factor of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87d413e-2873-454e-9ef8-54d5ff1a700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b132e8f2-71aa-4c96-ab2d-c73d7d20db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e10b68-1627-428d-b4e5-0e177817a367",
   "metadata": {},
   "source": [
    "## Define device (CPU or GPU)\n",
    "This notebook is run on a MacBook Pro with Apple M2 Max chip. Thus, device is set to \"mps\" for GPU. For other hardware architecture use:\n",
    "\n",
    "```Python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a800060d-a6ff-47d4-9be4-414685d23dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50baa8-bfaf-4b56-917d-e18e5ba50abb",
   "metadata": {},
   "source": [
    "## Load the dataset of movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8796edf8-f768-4b18-980c-b49306a520ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples is: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "print(f'Number of examples is: {len(df)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41a9e99-5d77-43b0-a14a-903f7fc70b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.sample(n = 1000, random_state=42)  # run on a small subset of data to test code\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac57323-b4c7-4434-9452-d30a1807340e",
   "metadata": {},
   "source": [
    "## Load BERT tokenizer and model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ad35e6-3c04-4c5a-a9b9-3958983c7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 labels: positive and negative\n",
    "# The informational message below indicates that \n",
    "# the model's output layer, including the logits, is randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee40d6b-ee74-4fc1-9e43-c5ee13a74227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Inspect model config\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351cdd00-e01f-4d43-bfdc-b6e355e22397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the maximum sequence length\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730e2a72-16ef-4196-acbf-8b698b3da532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:53<00:00, 438.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>sentiment_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[101, 2028, 1997, 1996, 2060, 15814, 2038, 385...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[101, 1037, 6919, 2210, 2537, 1012, 1026, 7987...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[101, 1045, 2245, 2023, 2001, 1037, 6919, 2126...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[101, 10468, 2045, 1005, 1055, 1037, 2155, 207...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[101, 9004, 3334, 4717, 7416, 1005, 1055, 1000...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 2028, 1997, 1996, 2060, 15814, 2038, 385...   \n",
       "1  [101, 1037, 6919, 2210, 2537, 1012, 1026, 7987...   \n",
       "2  [101, 1045, 2245, 2023, 2001, 1037, 6919, 2126...   \n",
       "3  [101, 10468, 2045, 1005, 1055, 1037, 2155, 207...   \n",
       "4  [101, 9004, 3334, 4717, 7416, 1005, 1055, 1000...   \n",
       "\n",
       "                                      attention_mask  sentiment_bool  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               1  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               1  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               1  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               0  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to tokenize and encode the text and return both input_ids and attention_mask\n",
    "def tokenize_and_encode(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256,\n",
    "        return_attention_mask=True,  # Return attention_mask\n",
    "    )\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply the function to the \"review\" column\n",
    "df[['input_ids', 'attention_mask']] = df['review'].progress_apply(lambda x: tokenize_and_encode(x)).apply(pd.Series)\n",
    "\n",
    "# Convert sentiments to boolean values\n",
    "df[\"sentiment_bool\"] = df[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd067b-e8d2-461e-93e4-b69adf7c6597",
   "metadata": {},
   "source": [
    "## Train / Validate / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c339b80-5761-4208-8e51-ac229ebfa53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b80e80-67f7-412a-a595-f8872484a3db",
   "metadata": {},
   "source": [
    "## Convert data to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a20a4a-2c97-4cc6-aa7d-6b6efe041ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_data(data):\n",
    "    out_input_ids = torch.tensor(data[\"input_ids\"].tolist())\n",
    "    out_attention_masks = torch.tensor(data[\"attention_mask\"].tolist())\n",
    "    out_labels = torch.tensor(data[\"sentiment_bool\"].tolist())\n",
    "    return out_input_ids, out_attention_masks, out_labels\n",
    "\n",
    "train_input_ids, train_attention_masks, train_labels = get_tensor_data(train_df)\n",
    "val_input_ids, val_attention_masks, val_labels = get_tensor_data(val_df)\n",
    "test_input_ids, test_attention_masks, test_labels = get_tensor_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baabc609-2b93-4558-96c4-fa5af5354d7d",
   "metadata": {},
   "source": [
    "## Create DataLoader for efficient batch processing\n",
    "When you create a DataLoader with shuffle=True, it shuffles the data once at the beginning when the DataLoader is created for that epoch. So, within a single epoch, the order of batches remains the same, but when you start a new epoch, the data is shuffled again, leading to a different order of examples for each epoch. This is a common practice in training deep learning models to ensure that the model doesn't memorize the order of examples and generalizes well across different batches and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f80dc15-ad6d-4214-8cc4-74455d741075",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1f41cd6-5f5f-48bf-9b27-0c8dbf55afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b6dfe-ff1c-45e9-9d2d-3f6b59651495",
   "metadata": {},
   "source": [
    "## Define optimizer and loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee4d1f6-f3ae-49fb-8ca7-d3da18e6353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Note: setting output_attentions and output_hidden_states to False can help with efficiency\n",
    "# because it reduces the amount of additional information that the model needs to compute and\n",
    "# store during forward passes. \n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels = 2, \n",
    "                                                      output_attentions = False, \n",
    "                                                      output_hidden_states = False)\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "# weight_decay (float, optional) – weight decay coefficient (default: 1e-2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) \n",
    "criterion = nn.CrossEntropyLoss() # binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d59159f-cf83-4f3f-bde1-8f350a46970e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_dropout_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21485775-6110-420d-9393-67783ebdeb75",
   "metadata": {},
   "source": [
    "## Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc01554-7c1c-47ab-a2a1-b686711bcda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to the device (GPU), which is 'mps' on Mac M2\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "526fb850-1d0c-4581-84e0-011c7e21a17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████                                                                           | 1/4 [22:38<1:07:54, 1358.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: Avg Loss 0.2471, Train Acc 0.8990, Val Acc 0.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████                                                   | 2/4 [45:10<45:09, 1354.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: Avg Loss 0.1423, Train Acc 0.9479, Val Acc 0.9252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████████████                         | 3/4 [1:08:07<22:44, 1364.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: Avg Loss 0.0778, Train Acc 0.9738, Val Acc 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [1:30:56<00:00, 1364.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: Avg Loss 0.0452, Train Acc 0.9864, Val Acc 0.9256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "torch.manual_seed(12345)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch in train_loader:\n",
    "        num_batches = len(train_loader)\n",
    "        inputs, attention_mask, labels = batch\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs_dict = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = criterion(outputs_dict.logits, labels)\n",
    "        predicted = outputs_dict.logits.argmax(dim=1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # Increment the number of correct predictions\n",
    "        correct += (predicted == labels).type(torch.float).sum().item()\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    \n",
    "    # validate\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_mask, labels = batch\n",
    "            inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs_dict = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "            val_predicted = outputs_dict.logits.argmax(dim=1)\n",
    "            val_correct += (val_predicted == labels).type(torch.float).sum().item()\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'epoch {epoch + 1}: Avg Loss {avg_train_loss:.4f}, Train Acc {train_acc:.4f}, Val Acc {val_acc:.4f}')          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb2b2ea4-aea7-40d6-a0ca-b9d6e65e0c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: Avg Loss 0.0452, Train Acc 0.9864, Val Acc 0.9256, Test Acc: 0.9226\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs_dict = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        test_predicted = outputs_dict.logits.argmax(dim=1)\n",
    "        test_correct += (test_predicted == labels).type(torch.float).sum().item()\n",
    "test_acc = test_correct / len(test_loader.dataset)\n",
    "\n",
    "print(f'epoch {epoch + 1}: Avg Loss {avg_train_loss:.4f}, Train Acc {train_acc:.4f}, Val Acc {val_acc:.4f}, Test Acc: {test_acc:.4f}')          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bb0a16a-5500-471e-842b-f5c030c7a4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 109483778\n"
     ]
    }
   ],
   "source": [
    "# number of model parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {num_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
