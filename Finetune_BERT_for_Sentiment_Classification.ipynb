{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b69d7f56-7ac3-4a40-9c50-a7084ba0f6cd",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "[IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/)\n",
    "\n",
    "This is a dataset for binary sentiment classification, where each example contains a movie review along with its positive or negative sentiment label.\n",
    "\n",
    "We split the dataset into train (80%), validate (10%), and test (10%) sets. \n",
    "\n",
    "## Finetuning BERT\n",
    "\n",
    "`bert-base-uncased` is one of the pre-trained models from the BERT (Bidirectional Encoder Representations from Transformers) family of models developed by Google AI. It is a widely used variant of BERT and has been pre-trained on a massive amount of text data, including a mixture of books, articles, and web pages.\n",
    "\n",
    "Note that different BERT model variants may have different maximum sequence lengths, so be sure to understand which `max_length` you are working with. Generally, `max_length = 512` is the default for BERT models. \n",
    "\n",
    "Hyperparameter choices in this notebook:\n",
    "- Maximum Sequence Length (`max_length`) = 128\n",
    "- Batch size (`batch_size`) = 64\n",
    "- Optimizer: `AdamW`\n",
    "- Learning rate (`lr`) = 2e-5\n",
    "- Weight decay (`weight_decay`) = 0.01\n",
    "- Number of epochs (`epochs`) = 5\n",
    "\n",
    "## Optimization\n",
    "\n",
    "[`Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) and [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) algorithms share some similarities, but they also have important differences, mainly related to the handling of weight decay (L2 regularization) in training.\n",
    "\n",
    "- **`Adam`**: The original `Adam` optimizer includes weight decay as part of the parameter update step. Weight decay is added to the gradients of the model weights during the optimization process. This can lead to unintended behavior, such as incorrect weight decay being applied to some parameters.\n",
    "- **`AdamW`**: `AdamW` is a variant of `Adam` that explicitly decouples weight decay from the optimization step. Weight decay is applied directly to the model weights after the gradients are computed, making it more straightforward and predictable. This decoupling of weight decay is considered a more mathematically principled approach and is less prone to issues like incorrect weight decay scaling.\n",
    "\n",
    "In practice, the choice between `Adam` and `AdamW` depends on the specific task and model architecture. When working with models like `BERT` or other transformers, `AdamW` is often preferred due to its more reliable handling of weight decay. However, for simpler models and tasks, `Adam` may still be suitable.\n",
    "\n",
    "Here, we use `AdamW` with the default value for `weight_decay`, which is 0.01.\n",
    "\n",
    "## Results\n",
    "We obtain these metrics on the last epoch:\n",
    "- `epoch 5: Avg Loss 0.0420, Train Acc 0.9864, Val Acc 0.8902, Test Acc: 0.8874`\n",
    "\n",
    "Looking at the full results (scroll down), we could perhaps \"early-stop\" after the 2nd epoch to minimize overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87d413e-2873-454e-9ef8-54d5ff1a700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b132e8f2-71aa-4c96-ab2d-c73d7d20db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e10b68-1627-428d-b4e5-0e177817a367",
   "metadata": {},
   "source": [
    "## Define device (CPU or GPU)\n",
    "This notebook is run on a MacBook Pro with Apple M2 Max chip. Thus, device is set to \"mps\" for GPU. For other hardware architecture use:\n",
    "\n",
    "```Python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a800060d-a6ff-47d4-9be4-414685d23dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50baa8-bfaf-4b56-917d-e18e5ba50abb",
   "metadata": {},
   "source": [
    "## Load the dataset of movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8796edf8-f768-4b18-980c-b49306a520ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples is: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "print(f'Number of examples is: {len(df)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41a9e99-5d77-43b0-a14a-903f7fc70b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.sample(n = 1000, random_state=42)\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac57323-b4c7-4434-9452-d30a1807340e",
   "metadata": {},
   "source": [
    "## Load BERT tokenizer and model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ad35e6-3c04-4c5a-a9b9-3958983c7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 labels: positive and negative\n",
    "# The informational message below indicates that \n",
    "# the model's output layer, including the logits, is randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee40d6b-ee74-4fc1-9e43-c5ee13a74227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Inspect model config\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351cdd00-e01f-4d43-bfdc-b6e355e22397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the maximum sequence length\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b4983b-3e2e-4904-b1a3-ba7f5a88b39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the dropout rate applied to the hidden layers of BERT\n",
    "bert_config = BertConfig.from_pretrained(model_name)\n",
    "bert_config.hidden_dropout_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b700d-a5cd-4047-b7bc-4c71ae95e489",
   "metadata": {},
   "source": [
    "## Tokenize sentiments\n",
    "\n",
    "Let's break down the following code:\n",
    "```python\n",
    "tokenizer.encode(x, add_special_tokens=True, truncation=True, padding='max_length', max_length=128, return_attention_mask=True)\n",
    "```\n",
    "\n",
    "- `add_special_tokens=True`: This parameter instructs the tokenizer to add special tokens to the encoded sequence. In the case of BERT, these special tokens typically include `[CLS]` (the classification token) at the beginning of the sequence and `[SEP]` (the separator token) at the end of the sequence. These special tokens are important for the model to understand the structure of the input and to perform tasks like classification. For example, `[CLS]` is used to represent the start of the input, and `[SEP]` is used to separate segments in a sequence.\n",
    "\n",
    "- `truncation=True`: This parameter indicates that if a sequence is longer than the specified `max_length`, it should be truncated to fit within the limit. Truncation is necessary to ensure that sequences don't exceed the model's token limit (e.g., 128 tokens in your case).\n",
    "\n",
    "- `padding='max_length'`: This parameter specifies how to handle padding of sequences. When set to `'max_length'`, the tokenizer will pad sequences to the specified `max_length`. Padding is necessary because BERT models expect input sequences of equal length. By padding shorter sequences with special padding tokens (`[PAD]`), you ensure that all sequences have the same length as specified by `max_length`.\n",
    "\n",
    "- `max_length=128`: This argument specifies the maximum length of the output sequence after encoding. If the input sequence is longer than this value and truncation is set to True, it will be truncated to fit within this length. If it's shorter, it will be padded to match this length.\n",
    "  \n",
    "- `return_attention_mask`: When set to `True`, this argument instructs the tokenizer to return an attention mask along with the encoded input. An attention mask is a binary mask that indicates which tokens in the input sequence the model should pay attention to (i.e., not consider padding tokens). It helps the model focus on the actual content of the sequence and ignore padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730e2a72-16ef-4196-acbf-8b698b3da532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:53<00:00, 439.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>sentiment_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[101, 2028, 1997, 1996, 2060, 15814, 2038, 385...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[101, 1037, 6919, 2210, 2537, 1012, 1026, 7987...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[101, 1045, 2245, 2023, 2001, 1037, 6919, 2126...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[101, 10468, 2045, 1005, 1055, 1037, 2155, 207...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[101, 9004, 3334, 4717, 7416, 1005, 1055, 1000...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 2028, 1997, 1996, 2060, 15814, 2038, 385...   \n",
       "1  [101, 1037, 6919, 2210, 2537, 1012, 1026, 7987...   \n",
       "2  [101, 1045, 2245, 2023, 2001, 1037, 6919, 2126...   \n",
       "3  [101, 10468, 2045, 1005, 1055, 1037, 2155, 207...   \n",
       "4  [101, 9004, 3334, 4717, 7416, 1005, 1055, 1000...   \n",
       "\n",
       "                                      attention_mask  sentiment_bool  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               1  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               1  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               1  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               0  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...               1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to tokenize and encode the text and return both input_ids and attention_mask\n",
    "def tokenize_and_encode(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_attention_mask=True,  # Return attention_mask\n",
    "    )\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply the function to the \"review\" column\n",
    "df[['input_ids', 'attention_mask']] = df['review'].progress_apply(lambda x: tokenize_and_encode(x)).apply(pd.Series)\n",
    "\n",
    "# Convert sentiments to boolean values\n",
    "df[\"sentiment_bool\"] = df[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd067b-e8d2-461e-93e4-b69adf7c6597",
   "metadata": {},
   "source": [
    "## Train / Validate / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c339b80-5761-4208-8e51-ac229ebfa53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b80e80-67f7-412a-a595-f8872484a3db",
   "metadata": {},
   "source": [
    "## Convert data to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a20a4a-2c97-4cc6-aa7d-6b6efe041ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_data(data):\n",
    "    out_input_ids = torch.tensor(data[\"input_ids\"].tolist())\n",
    "    out_attention_masks = torch.tensor(data[\"attention_mask\"].tolist())\n",
    "    out_labels = torch.tensor(data[\"sentiment_bool\"].tolist())\n",
    "    return out_input_ids, out_attention_masks, out_labels\n",
    "\n",
    "train_input_ids, train_attention_masks, train_labels = get_tensor_data(train_df)\n",
    "val_input_ids, val_attention_masks, val_labels = get_tensor_data(val_df)\n",
    "test_input_ids, test_attention_masks, test_labels = get_tensor_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baabc609-2b93-4558-96c4-fa5af5354d7d",
   "metadata": {},
   "source": [
    "## Create DataLoader for efficient batch processing\n",
    "When you create a DataLoader with shuffle=True, it shuffles the data once at the beginning when the DataLoader is created for that epoch. So, within a single epoch, the order of batches remains the same, but when you start a new epoch, the data is shuffled again, leading to a different order of examples for each epoch. This is a common practice in training deep learning models to ensure that the model doesn't memorize the order of examples and generalizes well across different batches and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f80dc15-ad6d-4214-8cc4-74455d741075",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f41cd6-5f5f-48bf-9b27-0c8dbf55afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b6dfe-ff1c-45e9-9d2d-3f6b59651495",
   "metadata": {},
   "source": [
    "## Define optimizer and loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee4d1f6-f3ae-49fb-8ca7-d3da18e6353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Note: setting output_attentions and output_hidden_states to False can help with efficiency\n",
    "# because it reduces the amount of additional information that the model needs to compute and\n",
    "# store during forward passes. \n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels = 2, \n",
    "                                                      output_attentions = False, \n",
    "                                                      output_hidden_states = False)\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "# weight_decay (float, optional) – weight decay coefficient (default: 1e-2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)  # use default weight_decay\n",
    "criterion = nn.CrossEntropyLoss() # binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d59159f-cf83-4f3f-bde1-8f350a46970e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_dropout_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21485775-6110-420d-9393-67783ebdeb75",
   "metadata": {},
   "source": [
    "## Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fc01554-7c1c-47ab-a2a1-b686711bcda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to the device (GPU), which is 'mps' on Mac M2\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "526fb850-1d0c-4581-84e0-011c7e21a17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████▌                                                                                          | 1/5 [10:46<43:04, 646.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: Avg Loss 0.3238, Train Acc 0.8562, Val Acc 0.8794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████▏                                                                   | 2/5 [21:34<32:22, 647.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: Avg Loss 0.2009, Train Acc 0.9195, Val Acc 0.8950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████▊                                             | 3/5 [32:16<21:29, 644.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: Avg Loss 0.1201, Train Acc 0.9565, Val Acc 0.8962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████▍                      | 4/5 [42:57<10:43, 643.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: Avg Loss 0.0660, Train Acc 0.9774, Val Acc 0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [53:34<00:00, 642.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: Avg Loss 0.0420, Train Acc 0.9864, Val Acc 0.8902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "torch.manual_seed(12345)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch in train_loader:\n",
    "        num_batches = len(train_loader)\n",
    "        inputs, attention_mask, labels = batch\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs_dict = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = criterion(outputs_dict.logits, labels)\n",
    "        predicted = outputs_dict.logits.argmax(dim=1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # Increment the number of correct predictions\n",
    "        correct += (predicted == labels).type(torch.float).sum().item()\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    \n",
    "    # validate\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_mask, labels = batch\n",
    "            inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs_dict = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "            val_predicted = outputs_dict.logits.argmax(dim=1)\n",
    "            val_correct += (val_predicted == labels).type(torch.float).sum().item()\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'epoch {epoch + 1}: Avg Loss {avg_train_loss:.4f}, Train Acc {train_acc:.4f}, Val Acc {val_acc:.4f}')          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb2b2ea4-aea7-40d6-a0ca-b9d6e65e0c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: Avg Loss 0.0420, Train Acc 0.9864, Val Acc 0.8902, Test Acc: 0.8874\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs_dict = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        test_predicted = outputs_dict.logits.argmax(dim=1)\n",
    "        test_correct += (test_predicted == labels).type(torch.float).sum().item()\n",
    "test_acc = test_correct / len(test_loader.dataset)\n",
    "\n",
    "print(f'epoch {epoch + 1}: Avg Loss {avg_train_loss:.4f}, Train Acc {train_acc:.4f}, Val Acc {val_acc:.4f}, Test Acc: {test_acc:.4f}')          \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
